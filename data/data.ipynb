{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastF1 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fastf1 pandas scikit-learn torch\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Bahrain Grand Prix - Qualifying [v3.3.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '1', '55', '11', '44', '77', '20', '14', '63', '10', '31', '47', '4', '23', '24', '22', '27', '3', '18', '6']\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.3.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '55', '44', '63', '20', '77', '31', '22', '14', '24', '47', '18', '23', '3', '4', '6', '27', '11', '1', '10']\n"
     ]
    }
   ],
   "source": [
    "# event_schedule -> event -> session -> laps -> telemetry\n",
    "def process_event(event, year):\n",
    "    fastest_qualifying_lap_times = {}\n",
    "\n",
    "    qualifying = event.get_qualifying()\n",
    "    qualifying.load()\n",
    "\n",
    "    for driver in pd.unique(qualifying.results[\"Abbreviation\"]):\n",
    "        fastest_qualifying_lap_times[driver] = qualifying.laps.pick_driver(driver).pick_fastest().LapTime\n",
    "\n",
    "    race = event.get_race()\n",
    "    race.load()\n",
    "\n",
    "    race_laps = race.laps.reset_index(drop=True)\n",
    "    weather_data = race.laps.get_weather_data().reset_index(drop=True)\n",
    "    race_laps = pd.concat([race_laps, weather_data.loc[:, ~(weather_data.columns == 'Time')]], axis=1)\n",
    "\n",
    "    race_laps = race_laps[[\n",
    "        \"LapTime\",\n",
    "        \"Driver\",\n",
    "        \"LapNumber\", # TODO does this need to be normalized\n",
    "        \"Stint\",\n",
    "        \"Compound\",\n",
    "        \"TyreLife\",\n",
    "        \"Team\",\n",
    "        \"TrackStatus\",\n",
    "        \"Position\",\n",
    "        \"Rainfall\",\n",
    "        \"AirTemp\",\n",
    "        \"TrackTemp\"\n",
    "    ]]\n",
    "\n",
    "    # Qualifying results\n",
    "    race_laps[\"FastestQualifyingLapTime\"] = race_laps.apply(lambda row: fastest_qualifying_lap_times[row[\"Driver\"]], axis=1)\n",
    "    race_laps[\"FastestQualifyingLapTime\"] = race_laps[\"FastestQualifyingLapTime\"].dt.total_seconds() * 1000\n",
    "\n",
    "    # Circuit and year\n",
    "    race_laps[\"Event\"] = event.EventName\n",
    "    race_laps[\"Year\"] = year\n",
    "\n",
    "    return race_laps\n",
    "\n",
    "\n",
    "event = fastf1.get_event(2022, 'Bahrain')\n",
    "data = process_event(event, 2022)\n",
    "data.to_csv(\"bahrain-2022.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for year in range(2019, 2024):\n",
    "    event_schedule = fastf1.get_event_schedule(year, include_testing=False)\n",
    "\n",
    "    event_round = 0\n",
    "    while True:\n",
    "        event_round += 1\n",
    "\n",
    "        try:\n",
    "            event = event_schedule.get_event_by_round(event_round)\n",
    "            event_data = process_event(event, year)\n",
    "\n",
    "            data = pd.concat([data, event_data])\n",
    "        except:\n",
    "            break\n",
    "\n",
    "event_data = data\n",
    "\n",
    "# Process time data before saving as CSV\n",
    "event_data[\"LapTime\"] = event_data[\"LapTime\"].dt.total_seconds() * 1000\n",
    "event_data[\"FastestQualifyingLapTime\"] = event_data[\"FastestQualifyingLapTime\"].dt.total_seconds() * 1000\n",
    "\n",
    "event_data.to_csv(\"event-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = pd.read_csv(\"event-data.csv\").iloc[:, 1:]\n",
    "processed_data = event_data\n",
    "\n",
    "# One-hot encode driver\n",
    "processed_data[\"Driver_Orig\"] = processed_data[\"Driver\"]\n",
    "processed_data = pd.get_dummies(processed_data, columns=[\"Driver\"], dtype=int)\n",
    "\n",
    "# One-hot encode team\n",
    "processed_data = pd.get_dummies(processed_data, columns=[\"Team\"], dtype=int)\n",
    "\n",
    "# One-hot encode position\n",
    "processed_data = pd.get_dummies(processed_data, columns=[\"Position\"], dtype=int)\n",
    "\n",
    "# One-hot encode compound\n",
    "processed_data = pd.get_dummies(processed_data, columns=[\"Compound\"], dtype=int)\n",
    "\n",
    "# One-hot encode event\n",
    "processed_data[\"Event_Orig\"] = processed_data[\"Event\"]\n",
    "processed_data = pd.get_dummies(processed_data, columns=[\"Event\"], dtype=int)\n",
    "\n",
    "# Convert Rainfall boolean to int\n",
    "processed_data[\"Rainfall\"] = processed_data[\"Rainfall\"].astype(int)\n",
    "\n",
    "# Encode track status\n",
    "def track_status_apply(row):\n",
    "    track_status = str(row[\"TrackStatus\"])\n",
    "\n",
    "    clear = \"1\" in track_status\n",
    "    yellow = \"2\" in track_status\n",
    "    safety_car = \"4\" in track_status\n",
    "    red_flag = \"5\" in track_status\n",
    "    virtual_safety_car_deployed = \"6\" in track_status\n",
    "    virtual_safety_car_ending = \"7\" in track_status\n",
    "\n",
    "    encoding = [clear, yellow, safety_car, red_flag, virtual_safety_car_deployed, virtual_safety_car_ending]\n",
    "    return [float(i) for i in encoding]\n",
    "\n",
    "processed_data[[\n",
    "    \"TrackStatus_Clear\",\n",
    "    \"TrackStatus_YellowFlag\",\n",
    "    \"TrackStatus_SafetyCar\",\n",
    "    \"TrackStatus_RedFlag\",\n",
    "    \"TrackStatus_VirtualSafetyCarDeployed\",\n",
    "    \"TrackStatus_VirtualSafetyCarEnding\"\n",
    "]] = processed_data.apply(track_status_apply, axis=1, result_type=\"expand\")\n",
    "processed_data = processed_data.drop(columns=[\"TrackStatus\"])\n",
    "\n",
    "# Save processed data\n",
    "processed_data.to_csv(\"processed-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed-data.csv\").iloc[:, 1:]\n",
    "    \n",
    "def generate_ngrams(df, n, columns_to_drop=['Driver_Orig', 'Event_Orig']):\n",
    "    \"\"\"generate_ngrams creates n-grams from the input dataframe of consecutive laps\n",
    "    from a driver at a single event.\n",
    "\n",
    "    columns_to_drop: list of columns to drop from the n-gram (not tensor friendly)\n",
    "\n",
    "    Returns a Pandas dataframe of the n-grams\n",
    "    \"\"\"\n",
    "    ngrams_list = []\n",
    "\n",
    "    grouped = df.groupby(['Year', 'Event_Orig', 'Driver_Orig'])\n",
    "    for _, group in grouped:\n",
    "        if columns_to_drop:\n",
    "            group = group.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "        sorted_group = group.sort_values(by='LapNumber')\n",
    "\n",
    "        for i in range(len(sorted_group) - n + 1):\n",
    "            potential_ngram = sorted_group.iloc[i:i + n]\n",
    "\n",
    "            lap_numbers = potential_ngram['LapNumber'].to_list()\n",
    "            # Check if the n-gram laps are consecutive\n",
    "            if lap_numbers == list(range(int(min(lap_numbers)), int(min(lap_numbers)) + n)):\n",
    "                ngrams_list.append(potential_ngram.values.flatten())\n",
    "\n",
    "    col_names = [f'{col}_{i+1}' for i in range(n) for col in group]\n",
    "    ngrams_df = pd.DataFrame(ngrams_list, columns=col_names)\n",
    "\n",
    "    return ngrams_df\n",
    "\n",
    "\n",
    "ngrams_df = generate_ngrams(df, 5)\n",
    "ngrams_df.to_csv(\"ngrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105664, 620])\n"
     ]
    }
   ],
   "source": [
    "ngrams_df = pd.read_csv(\"ngrams.csv\").iloc[:, 1:]\n",
    "tensor = torch.tensor(ngrams_df.values, dtype=torch.float32)\n",
    "print(tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
